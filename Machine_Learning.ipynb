{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import streamlit as st\n",
    "import pandas as pd\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50496380",
   "metadata": {},
   "source": [
    "### Website: \n",
    "https://machinelearning-g7zjenvie2g438jznnjdig.streamlit.app/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50d31edb",
   "metadata": {},
   "source": [
    "These lines import the required modules and libraries for the Streamlit application. The DecisionTreeClassifier, LogisticRegression, and KNeighborsClassifier machine learning models, as well as Matplotlib, are used to build plots. Streamlit is used to build the web interface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "34c665af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeltaGenerator()"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "st.header(\"Maternal Health risk High risk, Mid risk and Low risk predictions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81672dc4",
   "metadata": {},
   "source": [
    "This line sets the header title for your Streamlit app."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "b9d15dab",
   "metadata": {},
   "outputs": [],
   "source": [
    "health_data = pd.read_csv(\"Maternal Health Risk Data Set.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71242f63",
   "metadata": {},
   "source": [
    "This line reads the \"Maternal Health Risk Data Set.csv\" CSV file into a data analysis tool called a pandas DataFrame.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "26547114",
   "metadata": {},
   "outputs": [],
   "source": [
    "describe = health_data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b416cf3",
   "metadata": {},
   "source": [
    "This line computes descriptive statistics for the DataFrame, such as mean, standard deviation, minimum, maximum, etc., and stores the result in the describe variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "459c2dfe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeltaGenerator()"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "st.subheader(\"Data Description\", divider=\"blue\")\n",
    "st.dataframe(describe)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "637ba039",
   "metadata": {},
   "source": [
    "These lines create a subheader and display the descriptive statistics in a table format with a blue divider line."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5d4c369",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "b6360498",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeltaGenerator()"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "st.subheader(\"Overview of the data\", divider=\"blue\")\n",
    "st.dataframe(health_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6b61ba7",
   "metadata": {},
   "source": [
    "These lines create another subheader and display the entire health data in a table format with a blue divider line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "0887d17b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x2880974d0>"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.scatter(health_data.Age, health_data.HeartRate, color=\"red\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1bcae77",
   "metadata": {},
   "source": [
    "These lines create a scatter plot using matplotlib. It's plotting the \"Age\" and \"HeartRate\" columns from your data with red dots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "ba2c8e0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeltaGenerator()"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "st.subheader(\"Distribution between the Age and the Heart rate\", divider=\"blue\")\n",
    "st.pyplot(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4adfd53e",
   "metadata": {},
   "source": [
    "These lines create a subheader for the scatter plot and display the plot using st.pyplot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "8b113032",
   "metadata": {},
   "outputs": [],
   "source": [
    "st.subheader(\"input data\")\n",
    "input_data = health_data.drop(columns=\"RiskLevel\")\n",
    "output_data = health_data[\"RiskLevel\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "865a93d8",
   "metadata": {},
   "source": [
    "These lines create a subheader and separate your input data (all columns except \"RiskLevel\") and output data (only \"RiskLevel\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "e204d2b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeltaGenerator()"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "st.dataframe(input_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe5b9349",
   "metadata": {},
   "source": [
    "This line displays the input data in a table format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "6287ea21",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_train, input_test, output_train, output_test = train_test_split(input_data, output_data, test_size=0.3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7102b05e",
   "metadata": {},
   "source": [
    "These lines split your data into training and testing sets using train_test_split from scikit-learn. It reserves 30% of the data for testing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d939d075",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "31e78402",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/birate/Documents/AI/ML_env/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-7 {color: black;}#sk-container-id-7 pre{padding: 0;}#sk-container-id-7 div.sk-toggleable {background-color: white;}#sk-container-id-7 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-7 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-7 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-7 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-7 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-7 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-7 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-7 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-7 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-7 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-7 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-7 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-7 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-7 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-7 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-7 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-7 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-7 div.sk-item {position: relative;z-index: 1;}#sk-container-id-7 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-7 div.sk-item::before, #sk-container-id-7 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-7 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-7 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-7 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-7 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-7 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-7 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-7 div.sk-label-container {text-align: center;}#sk-container-id-7 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-7 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-7\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>KNeighborsClassifier()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-7\" type=\"checkbox\" checked><label for=\"sk-estimator-id-7\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">KNeighborsClassifier</label><div class=\"sk-toggleable__content\"><pre>KNeighborsClassifier()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "KNeighborsClassifier()"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_decisiontree = DecisionTreeClassifier()\n",
    "model_logisticr = LogisticRegression()\n",
    "model_KN = KNeighborsClassifier(n_neighbors=5)\n",
    "\n",
    "model_decisiontree.fit(input_train, output_train)\n",
    "model_logisticr.fit(input_train, output_train)\n",
    "model_KN.fit(input_train, output_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcdbd580",
   "metadata": {},
   "source": [
    "These lines create three different machine learning models (Decision Tree, Logistic Regression, K-Nearest Neighbors), initialize them, and fit them with the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "86f808b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_decisionTree = model_decisiontree.predict(input_test)\n",
    "predictions_logisticr = model_logisticr.predict(input_test)\n",
    "predictions_KN = model_KN.predict(input_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90fb2938",
   "metadata": {},
   "source": [
    "These lines make predictions using the fitted models on the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "27a6efb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "algorithms = st.radio(\"Select an algorithms\", [\"Decision Tree\", \"Logistic Regression\", \"KN neighrest Neighbour\", \"Compare\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f291c66",
   "metadata": {},
   "source": [
    "This line creates a radio button in the Streamlit app, allowing the user to select an algorithm (model) or choose to compare them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "e9fb726c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if algorithms == \"Decision Tree\":\n",
    "    st.header(\"Decision Tree\")\n",
    "\n",
    "    score_decisionTree = accuracy_score(output_test, predictions_decisionTree)\n",
    "\n",
    "# Round to two decimal places\n",
    "    rounded_score_decisionTree = round(score_decisionTree, 2)\n",
    "\n",
    "# Convert to a percentage\n",
    "    percentage_score_decisionTree = rounded_score_decisionTree * 100\n",
    "\n",
    "# Visualize the percentage in a Streamlit application\n",
    "    st.metric(label=\"Accuracy Percentage\", value=percentage_score_decisionTree)\n",
    "    st.write(\"Accuracy Score Visualization\")\n",
    "\n",
    "# Define labels for the pie chart\n",
    "    # Add more model names if needed\n",
    "    labels = [\"Decision Tree accuracy\", \"Not accurate\"]\n",
    "\n",
    "    # Define the percentage scores corresponding to the labels\n",
    "    sizes = [percentage_score_decisionTree,\n",
    "             100 - percentage_score_decisionTree]\n",
    "\n",
    "    # Define colors for the sections of the pie chart\n",
    "    colors = ['lightblue', 'lightgray']\n",
    "\n",
    "    # Create the pie chart\n",
    "    fig1, ax1 = plt.subplots()\n",
    "    ax1.pie(sizes, labels=labels, colors=colors,\n",
    "            autopct='%1.1f%%', startangle=90)\n",
    "    # Equal aspect ratio ensures that pie is drawn as a circle.\n",
    "    ax1.axis('equal')\n",
    "\n",
    "    # Display the pie chart in Streamlit\n",
    "    st.pyplot(plt)\n",
    "\n",
    "elif algorithms == \"Logistic Regression\":\n",
    "    st.header(\"Logic Regression\")\n",
    "    score_logisticr = accuracy_score(output_test, predictions_logisticr)\n",
    "    rounded_score_logisticr = round(score_logisticr, 3)\n",
    "    percentage_score_logisticr = rounded_score_logisticr * 100\n",
    "    st.metric(label=\"Accuracy Percentage\", value=percentage_score_logisticr)\n",
    "\n",
    "    st.write(\"Accuracy Score Visualization\")\n",
    "\n",
    "   # Define labels for the pie chart\n",
    "    # Add more model names if needed\n",
    "    labels = [\"Decision Tree accuracy\", \"Not accurate\"]\n",
    "\n",
    "    # Define the percentage scores corresponding to the labels\n",
    "    sizes = [percentage_score_logisticr,\n",
    "             100 - percentage_score_logisticr]\n",
    "\n",
    "    # Define colors for the sections of the pie chart\n",
    "    colors = ['lightgreen', 'lightgray']\n",
    "\n",
    "    # Create the pie chart\n",
    "    fig1, ax1 = plt.subplots()\n",
    "    ax1.pie(sizes, labels=labels, colors=colors,\n",
    "            autopct='%1.1f%%', startangle=90)\n",
    "    # Equal aspect ratio ensures that pie is drawn as a circle.\n",
    "    ax1.axis('equal')\n",
    "\n",
    "    # Display the pie chart in Streamlit\n",
    "    st.pyplot(plt)\n",
    "elif algorithms == \"KN neighrest Neighbour\":\n",
    "    st.header(\"KN neighrest Neighbour\")\n",
    "\n",
    "    score_KN = accuracy_score(output_test, predictions_KN)\n",
    "    rounded_score_KN = round(score_KN, 3)\n",
    "    percentage_score_KN = rounded_score_KN * 100\n",
    "    st.metric(label=\"Accuracy Percentage\", value=percentage_score_KN)\n",
    "\n",
    "    st.write(\"Accuracy Score Visualization\")\n",
    "\n",
    "   # Define labels for the pie chart\n",
    "    # Add more model names if needed\n",
    "    labels = [\"Decision Tree accuracy\", \"Not accurate\"]\n",
    "\n",
    "    # Define the percentage scores corresponding to the labels\n",
    "    sizes = [percentage_score_KN,\n",
    "             100 - percentage_score_KN]\n",
    "\n",
    "    # Define colors for the sections of the pie chart\n",
    "    colors = ['lightyellow', 'lightgray']\n",
    "\n",
    "    # Create the pie chart\n",
    "    fig1, ax1 = plt.subplots()\n",
    "    ax1.pie(sizes, labels=labels, colors=colors,\n",
    "            autopct='%1.1f%%', startangle=90)\n",
    "    # Equal aspect ratio ensures that pie is drawn as a circle.\n",
    "    ax1.axis('equal')\n",
    "\n",
    "    # Display the pie chart in Streamlit\n",
    "    st.pyplot(plt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60b26629",
   "metadata": {},
   "source": [
    "These conditions determine which algorithm the user chose before executing particular code blocks in accordance with their selection.\n",
    "\n",
    "Each condition's code blocks compute, display, and show accuracy scores using pie charts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "a7b73db8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_accuracy_scores():\n",
    "    # Calculate the accuracy scores for different models\n",
    "    score_logisticr = accuracy_score(output_test, predictions_logisticr)\n",
    "    score_KN = accuracy_score(output_test, predictions_KN)\n",
    "    score_decisionTree = accuracy_score(output_test, predictions_decisionTree)\n",
    "    accuracy_scores = [score_decisionTree, score_logisticr, score_KN]\n",
    "    models = ['Decision Tree', 'Logistic Regression', 'kN Nearest Neighbour']\n",
    "\n",
    "    # Create a bar chart\n",
    "    plt.bar(models, accuracy_scores, color=['blue', 'green', 'red'])\n",
    "    plt.xlabel('Model')\n",
    "    plt.ylabel('Accuracy Score')\n",
    "    plt.title('Comparison of Accuracy Scores for Different Models')\n",
    "    plt.ylim(0, 1)  # Set the y-axis range to be between 0 and 1 for accuracy.\n",
    "\n",
    "    # Display the accuracy scores on top of the bars.\n",
    "    for i, score in enumerate(accuracy_scores):\n",
    "        plt.text(i, score, f'{score:.2f}', ha='center', va='bottom')\n",
    "\n",
    "    # Show the bar chart in the Streamlit app\n",
    "    st.pyplot(plt)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8ce0259",
   "metadata": {},
   "source": [
    "This function calculates and displays the accuracy scores of all three models and compares them using a bar chart."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "54891f68",
   "metadata": {},
   "outputs": [],
   "source": [
    "if algorithms == \"Compare\":\n",
    "    st.header(\"Comparison of Accuracy Scores for Different Models\")\n",
    "    compare_accuracy_scores()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6de9be9b",
   "metadata": {},
   "source": [
    "The compare_accuracy_scores() method is invoked by this condition to display the comparison of accuracy scores if the user picked \"Compare\" as their action."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df868e81",
   "metadata": {},
   "source": [
    "## Introduction ##\n",
    "\n",
    "\n",
    "In order to guarantee the health of both expectant mothers and their unborn children, predicting maternal health risk is an essential component of healthcare. This is accomplished by using classification algorithms, such as Decision Trees, Logistic Regression, and K-Nearest Neighbors (KNN), to evaluate and divide maternal health risks into three distinct levels: high risk, mid risk, and low risk. For the purpose of making precise and timely forecasts, these algorithms examine a wide range of pertinent data, such as medical history, vital signs, and numerous health markers. \n",
    "\n",
    "\n",
    "### The different Algorithms ###\n",
    "\n",
    "KNN categorizes maternal health hazards based on their resemblance to neighboring cases, whereas Decision Trees create hierarchical decision rules, Logistic Regression models the probability of various risk levels, and these three techniques work together in this context. By utilizing these algorithms, healthcare professionals may give tailored care to expectant moms, allocate resources effectively, and make informed decisions, thereby improving maternal and newborn outcomes.\n",
    "\n",
    "### 1. Logistic Regression ### \n",
    "\n",
    "  Training: To maximize the probability of the observed data, the logistic regression model is trained to identify the values of the coefficients (0, 1, 2,...) that maximize that likelihood. Gradient descent and other optimization algorithms are frequently used for this. The coefficients of the model are changed iteratively until convergence is reached by minimizing a loss function, typically the log-likelihood.\n",
    "\n",
    "   Prediction: Once the model has been trained, predictions can be made using fresh data. The logistic regression model determines the likelihood that the binary result will be 1, given a collection of input features. The model forecasts an event as 1 if this probability is higher than a predetermined threshold (often 0.5), and as 0 otherwise.\n",
    "\n",
    "### 2. KNN (K-Nearest Neighbors) ###\n",
    "\n",
    "Training: The method simply saves the full dataset throughout the training phase, including the input characteristics and their related labels.\n",
    "\n",
    "Distance Metric: To determine how similar two data points are, KNN uses a distance metric, such as the Euclidean distance, Manhattan distance, or another. The challenge and the type of data will determine the distance measure to choose.\n",
    "\n",
    "Prediction:\n",
    "\n",
    "In order to categorize a new data point, KNN locates the k-nearest data points in the training set using the selected distance measure. You have to pre-define the hyperparameter \"K\".\n",
    "\n",
    "The k-nearest neighbors are used in the procedure to count the number of data points in each class.\n",
    "\n",
    "\n",
    "\n",
    "## Explaination of the results \n",
    "\n",
    "Decision Trees: When tested with 30% of the data as the test set, the Decision Tree model consistently received the highest accuracy score, ranging from 70% to 80%. This implies that categorizing maternal health concerns using decision trees is a reliable strategy. In this situation, Decision Trees' hierarchical decision rules can be especially useful because they can identify intricate correlations in the data.\n",
    " \n",
    "K-Nearest Neighbors (KNN): The accuracy score for the KNN model was between 65% and 70%. Although it might not perform better than Decision Trees, KNN still offers a dependable prediction. KNN depends on the similarity of nearby data points, and the value of the hyperparameter \"K\" can affect how well it performs. The precision of this parameter might be increased with careful tweaking.\n",
    "\n",
    "The accuracy score for logistic regression: which ranged from 50% to 60%, was the lowest. Logistic Regression is nevertheless an effective tool for modeling probability and binary classification tasks, but perhaps not performing as well as the other two algorithms in this particular situation. Its performance might be enhanced by feature engineering or by taking into account more sophisticated variants of logistic regression models.\n",
    "\n",
    "\n",
    "## Conclusion: \n",
    "\n",
    "\n",
    "The specific needs and goals of the healthcare system should serve as a guide when selecting an algorithm to forecast maternal health concerns. Decision trees seem to be the most precise choice, making them appropriate for reliable risk assessment. KNN is also capable of making predictions that are reasonably accurate, which may be useful in some circumstances. Although less accurate in this instance, logistic regression nevertheless provides insightful information that may be applicable to other maternal healthcare-related topics.\n",
    "\n",
    "\n",
    "## use of AI (ChatGPT):\n",
    "\n",
    "    * Explain briefly Logistic Regression\n",
    "    * Explain briefly K-Nearest Neighbors\n",
    "    * make a graph to compare the accuracy of the 3 algorithms in streamlit\n",
    "\n",
    "    * How to use K-Nearest Neighbors in python machine learning\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
